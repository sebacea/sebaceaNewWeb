{
  "hash": "c239a264349a0a952f39e9eba1fe5891",
  "result": {
    "markdown": "---\ntitle: \"NBA draft clustering using dimensionality reduction in R\"\ndescription: |\n  Comparing PCA vs UMAP vs TSNE to analyze NBA draft data\nauthor:\n  - name: Karat Sidhu\n    url: {}\ndate: 2022-09-15\nimage: images/thumb.jpg\ncategories:\n  - Long-Read\n  - TidyModels\n  - Dimension Reduction\n  - PCA\n  - UMAP\n  - TSNE\ntoc: true\ntoc-title: Table of contents\ntoc-location: left\ndraft: true\n---\n\n\n\n# NBA Draft Analysis\n\n![main](images/draft.jpg)\n\nThis is a continuation of my series of learning to use different algorithms in R and learning more about the Tidymodels package, and its various uses. \n\n\n\n## About the Data\n\nThe data set used comes for Kaggle.com, a great source of finding all kinds of data sets for data visualization and analysis. The data contains various statistics for all the NBA players drafted into the league from 1989 to 2021. It is a fairly tidy data set and requires little to none data clean in most of the cases to use for analysis. \n\n\n:::{.callout-note}\n\n\nLink to the original kaggle.com source can be found [here](https://www.kaggle.com/datasets/mattop/nba-draft-basketball-player-data-19892021) or by copy pasting the following URL:\n\n> https://www.kaggle.com/datasets/mattop/nba-draft-basketball-player-data-19892021\n:::\n\n\n\nWith some additional information:\n\n:::{.callout-tip collapse=\"true\"}\n\n## Data Information (click to expand)\n\n\nThe dataset contains all NBA Draft picks from 1989-2021. Dataset consists of year, overall pick and player data.\n\nNotable players: LeBron James, Kobe Bryant, Derrick Rose, Dirk Nowitzki, Carmelo Anthony, Stephen Curry, Paul Pierce, Kevin Durant, Shaq, Vince Carter, Allen Iverson\n\nData from: https://www.basketball-reference.com/draft/\n\n:::\n\n\n![banner](images/banner.jpeg)\n\n## Motivation\n\nAs stated earlier, my primary motivation for this blog post is learning how to use various methods of clustering data to find relationship between different players and their careers, and look at how closely those careers might be related, and potentially what influences such relations. \n\nEach of the alogrithms/methods use work in a slightly different way, primarily unsupervised and it should hopefully show the differences in which all players are related. Furthermore, these differences will also be method dependent and hopefully interesting to a basketball or a NBA fan. \n\n## Algorithms used\n\nSome of the methods I hope to use in this blog post include:\n\n- PCA\n- UMAP\n- T-SNE\n\n\n## Packages Used\n\nFor all of the aforementioned algorithms, R-code was used to carry out the analysis. \n\nSome of the R-packages used for data analysis as well as data modelling include:\n\n- TidyModels - for PCA, UMAP and KNN\n- TidyVerse - for data wrangling, data tidying\n- RTSNE - for T-SNE\n\n\n## Data Analysis\n\n### Loading and Looking at the data\n\nData is available in a CSV format, and can be read and looked at using the readR package in R. \n\n#### Loading Libraries\n\nAdding all the libraries needed for all analysis in a single place. The main libraries used are mentioned in the previous section.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.1     ✔ rsample      1.1.0\n✔ dials        1.0.0     ✔ tune         1.0.0\n✔ infer        1.0.3     ✔ workflows    1.0.0\n✔ modeldata    1.0.0     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.1     ✔ yardstick    1.0.0\n✔ recipes      1.0.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidytext)\nlibrary(Rtsne)\nlibrary(embed)\n```\n:::\n\n\n#### Loading the CSV file\n\nThe data files were downloaded from the Kaggle repo and saved in the CSV format locally for ease of use. All original data files are also available on the Kaggle repo linked above.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnba <- read_csv(\"/Users/karatatiwantsinghsidhu/Documents/Code/karat_codes/posts/nba-clustering/data/nbaplayersdraft.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 1922 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): team, player, college\ndbl (21): id, year, rank, overall_pick, years_active, games, minutes_played,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nnba  |>  as_tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,922 × 24\n      id  year  rank overall…¹ team  player college years…² games minut…³ points\n   <dbl> <dbl> <dbl>     <dbl> <chr> <chr>  <chr>     <dbl> <dbl>   <dbl>  <dbl>\n 1     1  1989     1         1 SAC   Pervi… Louisv…      11   474   11593   4494\n 2     2  1989     2         2 LAC   Danny… Duke         13   917   18133   6439\n 3     3  1989     3         3 SAS   Sean … Arizona      12   742   24502  10544\n 4     4  1989     4         4 MIA   Glen … Michig…      15  1000   34985  18336\n 5     5  1989     5         5 CHH   J.R. … UNC          11   672   15370   5680\n 6     6  1989     6         6 CHI   Stace… Oklaho…       8   438    7406   2819\n 7     7  1989     7         7 IND   Georg… Florid…      12   766   17429   6925\n 8     8  1989     8         8 DAL   Randy… Louisi…       5   281    5382   2083\n 9     9  1989     9         9 WSB   Tom H… Georgi…      12   687   10419   3617\n10    10  1989    10        10 MIN   Pooh … UCLA         10   639   19399   7083\n# … with 1,912 more rows, 13 more variables: total_rebounds <dbl>,\n#   assists <dbl>, field_goal_percentage <dbl>, `3_point_percentage` <dbl>,\n#   free_throw_percentage <dbl>, average_minutes_played <dbl>,\n#   points_per_game <dbl>, average_total_rebounds <dbl>, average_assists <dbl>,\n#   win_shares <dbl>, win_shares_per_48_minutes <dbl>, box_plus_minus <dbl>,\n#   value_over_replacement <dbl>, and abbreviated variable names ¹​overall_pick,\n#   ²​years_active, ³​minutes_played\n```\n:::\n:::\n\n\n\n### Data Cleaning & Exploratory Analysis\n\nClean the data in the database and remove the old data. \n\nBefore cleaning the database, lets look in a bit more detail on how each data component is labelled.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(nba)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"id\"                        \"year\"                     \n [3] \"rank\"                      \"overall_pick\"             \n [5] \"team\"                      \"player\"                   \n [7] \"college\"                   \"years_active\"             \n [9] \"games\"                     \"minutes_played\"           \n[11] \"points\"                    \"total_rebounds\"           \n[13] \"assists\"                   \"field_goal_percentage\"    \n[15] \"3_point_percentage\"        \"free_throw_percentage\"    \n[17] \"average_minutes_played\"    \"points_per_game\"          \n[19] \"average_total_rebounds\"    \"average_assists\"          \n[21] \"win_shares\"                \"win_shares_per_48_minutes\"\n[23] \"box_plus_minus\"            \"value_over_replacement\"   \n```\n:::\n:::\n\n\nData is in the long format, so we don't need to transform the columns or rows and can use the given dataset as is. Furthermore, we don't need to use the ID column in the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnba <- nba  |> select(-c(id, college))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnba <- nba  |> na.omit()\n```\n:::\n\n\n\n\n\n\n## Data Modelling\n\nSet seed\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n```\n:::\n\n\n\n### PCA\n\nPrep  the recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_rec <- recipe(~., data = nba) |>  # what data to use\n  update_role(player,team, new_role = \"id\") |> # name and category are identifiers not variables\n  step_normalize(all_predictors()) |> # normalize all other columns\n  step_pca(all_predictors()) # pca for all other columns\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npca_prep <- prep(pca_rec)\n\npca_prep\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        id          2\n predictor         20\n\nTraining data contained 1529 data points and no missing data.\n\nOperations:\n\nCentering and scaling for year, rank, overall_pick, years_active, games, ... [trained]\nPCA extraction with year, rank, overall_pick, years_active, games, m... [trained]\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidied_pca <- tidy(pca_prep, 2)\n\ntidied_pca\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 400 × 4\n   terms                   value component id       \n   <chr>                   <dbl> <chr>     <chr>    \n 1 year                   0.0618 PC1       pca_EoYnc\n 2 rank                   0.172  PC1       pca_EoYnc\n 3 overall_pick           0.172  PC1       pca_EoYnc\n 4 years_active          -0.263  PC1       pca_EoYnc\n 5 games                 -0.277  PC1       pca_EoYnc\n 6 minutes_played        -0.293  PC1       pca_EoYnc\n 7 points                -0.291  PC1       pca_EoYnc\n 8 total_rebounds        -0.265  PC1       pca_EoYnc\n 9 assists               -0.250  PC1       pca_EoYnc\n10 field_goal_percentage -0.125  PC1       pca_EoYnc\n# … with 390 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidied_pca |> \n  filter(\n    component == \"PC1\" |\n      component == \"PC2\" |\n      component == \"PC3\" |\n      component == \"PC4\"\n   ) |> \n  mutate(component = fct_inorder(component)) |> \n    ggplot(aes(value, terms, fill = terms)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~component, nrow = 1) +\n  hrbrthemes::theme_ipsum() +\n  labs(y = NULL) \n```\n\n::: {.cell-output-display}\n![](nba-clustering_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidied_pca |> \n  filter(component %in% paste0(\"PC\", 1:2)) |> \n  group_by(component) |>\n  top_n(8, abs(value)) |>\n  ungroup() |>\n  mutate(terms = reorder_within(terms, abs(value), component)) |>\n  ggplot(aes(abs(value), terms, fill = value > 0)) +\n  geom_col() +\n  facet_wrap(~component, scales = \"free_y\") +\n  scale_y_reordered() +\n  labs(\n    x = \"Absolute value of contribution\",\n    y = NULL, fill = \"Positive?\"\n  ) +\n  hrbrthemes::theme_ipsum()\n```\n\n::: {.cell-output-display}\n![](nba-clustering_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\njuice(pca_prep) |> \n  ggplot(aes(PC1, PC2, label = player)) +\n  geom_point(aes(color = team),alpha = 0.7, size = 2) +\n  ggrepel::geom_text_repel(max.overlaps = 40) +\n  labs(color = NULL) + hrbrthemes::theme_ipsum()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: ggrepel: 1497 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n```\n:::\n\n::: {.cell-output-display}\n![](nba-clustering_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\n### UMAP\n\nUMAP Prep\n \n\n ::: {.cell}\n \n ```{.r .cell-code}\n umap_rec <- recipe(~., data = nba) |>\n  update_role(player, team, new_role = \"id\") |>\n  step_normalize(all_predictors()) |>\n  step_umap(all_predictors())\n ```\n :::\n\n::: {.cell}\n\n```{.r .cell-code}\numap_prep <- prep(umap_rec)\n\numap_prep\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        id          2\n predictor         20\n\nTraining data contained 1529 data points and no missing data.\n\nOperations:\n\nCentering and scaling for year, rank, overall_pick, years_active, games, ... [trained]\nUMAP embedding for year, rank, overall_pick, years_active, games,... [trained]\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\njuice(umap_prep) |> \n  ggplot(aes(UMAP1, UMAP2, label = player)) +\n  geom_point(aes(color = team), alpha = 0.7, size = 2) +\n  ggrepel::geom_text_repel(max.overlaps = 75) +\n  labs(color = NULL) + hrbrthemes::theme_ipsum()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: ggrepel: 1481 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n```\n:::\n\n::: {.cell-output-display}\n![](nba-clustering_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n### TSNE\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnba_tsne <- nba  |> select(-c(team, player))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntsne <- Rtsne(nba_tsne, \nperplexity = 30,\neta = 100,\nmax_iter = 1000)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nY <- as.data.frame(tsne$Y)\n\nteams <- nba$team\nplayers <- nba$player\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(Y, aes(x =V1, y =V2, label = players)) +\ngeom_point(aes(color = teams)) + labs(x = \"tsne-1\", y = \"tsne-2\",color = \"team\") +\n  ggrepel::geom_text_repel(max.overlaps = 60) + hrbrthemes::theme_ipsum()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: ggrepel: 1519 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n```\n:::\n\n::: {.cell-output-display}\n![](nba-clustering_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n\n## Results and Conclusion\n\nThree different results are available for each of algorithms through which the data is run. Using a smaller data set or knowing even more information like player positions (PF, C, PG etc.) could further help with the results and establishing a pattern of why each of the players are linked together or clustered together.\n\nNevertheless, it was an interesting exercise for me personally and trying out TSNE in R for the first time.\n\n\n\n\n## References\n\n\n## Additional Notes\n",
    "supporting": [
      "nba-clustering_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}